{"pageProps":{"body":{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Demonstration of the normal equation formula:\n","$\\hspace{1cm}$\n","0. *We define the hypothesis and cost function as follow*:\n","$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2$$ \n","$$\\theta = \\begin{bmatrix}\\theta_{0} \\\\\\theta_{1} \\\\... \\\\\\theta_{n} \\end{bmatrix},x = \\begin{bmatrix}x_{0}^{(1)} & x_{0}^{(2)} & ... & x_{0}^{(m)}\\\\x_{1}^{(1)} & x_{1}^{(2)} & ... & x_{1}^{(m)} \\\\. & . & . & . \\\\x_{n}^{(1)} & x_{n}^{(2)} & ... & x_{n}^{(m)} \\end{bmatrix}, h_{\\theta}(x) = \\theta^{T}x$$  \n","\n","- *Note*: Our objective is to find the value of $\\theta$ that solves the equation $\\frac{\\partial J}{\\partial \\theta} = 0$.\n","1. First we start by simplyfing the formula:\n","    - Remove the $\\frac{1}{2m}$ since it has no impact when deriving.\n","    - Eliminate the sum by introducing $(h - y)$ as a design matrix (Matrix that contains everything, it's shape will be $(n_{features}, m)$).\n","$$J(\\theta) = (\\begin{bmatrix} \\theta^Tx^{(1)} \\\\ \\theta^Tx^{(2)} \\\\ ... \\\\ \\theta^Tx^{(m)} \\end{bmatrix} - \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{bmatrix})Â²$$ \n","$$J(\\theta) = (\\begin{bmatrix} \\theta_0 x_0^{(1)} & \\theta_1 x_1^{(1)} & ... & \\theta_{n} x_n^{(1)} \\\\ \\theta_{0} x_0^{(2)} & \\theta_1x_1^{(2)} & ... & \\theta_{n} x_n^{(2)} \\\\ . & . & . & . \\\\ \\theta_0x_0^{(m)} & \\theta_1x_1^{(m)} & ... & \\theta_{n} x_n^{(m)}\\end{bmatrix} - \\begin{bmatrix}y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}\\end{bmatrix})^2$$\n","- *Note* that dimension of $x$ written this way becomes $m \\times n$ instead of $n \\times m$. This allows use to rewrite it in a more general form:\n","$$J(\\theta) = (X\\theta - Y)^2$$\n","2. Deploy the binomial square identity (Be careful, this is the matrix version so we have to transpose the first term in order to square all the numbers):\n","\n","$$J(\\theta) = (X\\theta - Y)^T(X\\theta - Y) = (X\\theta)^TX\\theta - (X\\theta)^TY - Y(X\\theta)^T + Y^TY$$\n","3. Sum the $2nd$ and $3rd$ terms, $(X\\theta)^T$ and $Y$ are vectors. if $A$ and $B$ are vector matrices, then $A^TB = B^TA$ (Take care of this information, you'll need it a lot when dealing with linear algebra in machine learning).\n","$$J(\\theta) = (X\\theta)^TX\\theta - 2(X\\theta)^TY + Y^TY$$\n","4. The formula as it is now cannot be more simplified. So we'll start searching for it's derivative with respect to $\\theta$:\n","$$\\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} - \\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} + \\frac{\\partial Y^TY}{\\partial \\theta}$$\n","$$\\frac{\\partial Y^TY}{\\partial \\theta} = 0$$\n","$$\\frac{\\partial 2(X\\theta)^TY}{\\partial \\theta} = 2X^TY$$ \n","$$\\frac{\\partial (X\\theta)^TX\\theta}{\\partial \\theta} = X^TX\\theta + X(X\\theta)^T = 2X^TX\\theta$$\n","5. The initial objective was to resolve $\\frac{\\partial J}{\\partial \\theta} = 0$, so we put each term together:\n","$$\\frac{\\partial J}{\\partial \\theta} = 2X^TX\\theta - 2X^TY = 0$$\n","$$2X^TX\\theta = 2X^TY$$\n","$$X^TX\\theta = X^TY$$\n","6. And finally, the fraction $\\frac{1}{A}$ where $A$ is a matrix, is equivalent to the inverse of $A$. The inverse of a matrix is noted $A^{-1}$ and $AA^{-1} = I$.\n","$$\\theta = (X^TX)^{-1}X^TY$$\n"],"auto_number":1}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}},"__N_SSG":true}